
# import json
# import os
# import time
# import uuid
# from typing import Dict, List

# import google.generativeai as genai
# import psycopg2
# from fastapi import (APIRouter, File, Form, UploadFile)
# from historiesapi import histories
# from PIL import Image
# from psycopg2.extras import RealDictCursor

# from .textfunc import call_gemini_with_retry
# from .textapi_qwen import search_products
# from config import settings

# def get_db():
#     return psycopg2.connect(**settings.DB_CONFIG)

# router = APIRouter()
# # ================================================================================================
# # FUNCTION DEFINITIONS
# # ================================================================================================
    
# def batch_classify_materials(materials_batch: List[Dict]) -> List[Dict]:
#     """
#     Ph√¢n lo·∫°i H√ÄNG LO·∫†T v·∫≠t li·ªáu
#     Input: [{'name': 'G·ªñ S·ªíI', 'id_sap': 'M001'}, ...]
#     Output: [{'id_sap': 'M001', 'material_group': 'G·ªó', ...}, ...]
#     """
#     if not materials_batch:
#         return []
    
#     # [FIX] ƒê·ªïi sang model gemini-1.5-flash ƒë·ªÉ ·ªïn ƒë·ªãnh h∆°n v√† tr√°nh l·ªói Rate Limit
#     model = genai.GenerativeModel("gemini-2.5-flash")
    
#     materials_text = ""
#     for i, mat in enumerate(materials_batch, 1):
#         materials_text += f"{i}. ID: {mat['id_sap']}, T√™n: {mat['name']}\n"
    
#     prompt = f"""
#                 Ph√¢n lo·∫°i {len(materials_batch)} nguy√™n v·∫≠t li·ªáu n·ªôi th·∫•t:
#                 {materials_text}
#                 X√°c ƒë·ªãnh:
#                 1. material_group: G·ªó, Da, V·∫£i, ƒê√°, Kim lo·∫°i, K√≠nh, Nh·ª±a, S∆°n, Keo, Ph·ª• ki·ªán, Kh√°c
#                 2. material_subgroup: Nh√≥m con c·ª• th·ªÉ (VD: "G·ªó t·ª± nhi√™n", "Da th·∫≠t", "V·∫£i cao c·∫•p")
#                 OUTPUT JSON ARRAY ONLY:
#                 [
#                     {{"id_sap": "M001", "material_group": "...", "material_subgroup": "..."}},
#                     {{"id_sap": "M002", "material_group": "...", "material_subgroup": "..."}}
#                 ]
#             """
    
#     # G·ªçi Gemini v·ªõi retry
#     response_text = call_gemini_with_retry(model, prompt, max_retries=3)
    
#     # T·∫°o k·∫øt qu·∫£ m·∫∑c ƒë·ªãnh (Fallback) ƒë·ªÉ tr·∫£ v·ªÅ n·∫øu AI l·ªói
#     default_results = [{
#         'id_sap': m['id_sap'],
#         'material_group': 'Ch∆∞a ph√¢n lo·∫°i',
#         'material_subgroup': 'Ch∆∞a ph√¢n lo·∫°i'
#     } for m in materials_batch]

#     if not response_text:
#         return default_results
    
#     try:
#         clean = response_text.strip()
#         # X·ª≠ l√Ω l√†m s·∫°ch markdown JSON
#         if "```json" in clean:
#             clean = clean.split("```json")[1].split("```")[0].strip()
#         elif "```" in clean:
#             clean = clean.split("```")[1].split("```")[0].strip()
        
#         results = json.loads(clean)
        
#         # Ki·ªÉm tra s·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ c√≥ kh·ªõp input kh√¥ng
#         if len(results) != len(materials_batch):
#             print(f"WARNING: Batch materials mismatch: expected {len(materials_batch)}, got {len(results)}")
#             return default_results
        
#         return results
        
#     except Exception as e:
#         print(f"ERROR: Batch materials classification error: {e}")
#         return default_results

# def batch_classify_products(products_batch: List[Dict]) -> List[Dict]:
#     """
#     Ph√¢n lo·∫°i H√ÄNG LO·∫†T s·∫£n ph·∫©m - 1 API call cho nhi·ªÅu s·∫£n ph·∫©m
#     Input: [{'name': 'B√ÄN G·ªñ', 'id_sap': 'SP001'}, ...]
#     Output: [{'id_sap': 'SP001', 'category': 'B√†n', ...}, ...]
#     """
#     if not products_batch:
#         return []
    
#     # [FIX] ƒê·ªïi sang model ·ªïn ƒë·ªãnh ƒë·ªÉ tr√°nh l·ªói Rate Limit c·ªßa b·∫£n Experimental
#     model = genai.GenerativeModel("gemini-2.5-flash")
    
#     # T·∫°o danh s√°ch s·∫£n ph·∫©m trong prompt
#     products_text = ""
#     for i, prod in enumerate(products_batch, 1):
#         products_text += f"{i}. ID: {prod['id_sap']}, T√™n: {prod['name']}\n"
    
#     prompt = f"""
#             B·∫°n l√† chuy√™n gia ph√¢n lo·∫°i s·∫£n ph·∫©m n·ªôi th·∫•t cao c·∫•p.
#             Ph√¢n lo·∫°i {len(products_batch)} s·∫£n ph·∫©m sau:
#             {products_text}
#             M·ªói s·∫£n ph·∫©m c·∫ßn ph√¢n lo·∫°i theo:
#             1. category: B√†n, Gh·∫ø, Sofa, T·ªß, Gi∆∞·ªùng, ƒê√®n, K·ªá, B√†n l√†m vi·ªác, Kh√°c
#             2. sub_category: Danh m·ª•c ph·ª• c·ª• th·ªÉ (VD: "B√†n ƒÉn", "Gh·∫ø bar", "Sofa g√≥c"...)
#             3. material_primary: G·ªó, Da, V·∫£i, Kim lo·∫°i, ƒê√°, K√≠nh, Nh·ª±a, M√¢y tre, H·ªón h·ª£p
#             OUTPUT JSON ARRAY ONLY (no markdown, no backticks):
#             [
#                 {{"id_sap": "SP001", "category": "...", "sub_category": "...", "material_primary": "..."}},
#                 {{"id_sap": "SP002", "category": "...", "sub_category": "...", "material_primary": "..."}}
#             ]
#     """
    
#     # G·ªçi AI v·ªõi retry logic
#     response_text = call_gemini_with_retry(model, prompt, max_retries=3)
    
#     # Fallback m·∫∑c ƒë·ªãnh n·∫øu AI l·ªói h·∫≥n
#     default_results = [{
#         'id_sap': p['id_sap'],
#         'category': 'Ch∆∞a ph√¢n lo·∫°i',
#         'sub_category': 'Ch∆∞a ph√¢n lo·∫°i',
#         'material_primary': 'Ch∆∞a x√°c ƒë·ªãnh'
#     } for p in products_batch]

#     if not response_text:
#         return default_results
    
#     try:
#         clean = response_text.strip()
#         # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p Gemini tr·∫£ v·ªÅ markdown code block
#         if "```json" in clean:
#             clean = clean.split("```json")[1].split("```")[0].strip()
#         elif "```" in clean:
#             clean = clean.split("```")[1].split("```")[0].strip()
        
#         results = json.loads(clean)
        
#         # ƒê·∫£m b·∫£o s·ªë l∆∞·ª£ng k·∫øt qu·∫£ kh·ªõp v·ªõi input
#         if len(results) != len(products_batch):
#             print(f"WARNING: Batch size mismatch: expected {len(products_batch)}, got {len(results)}")
#             return default_results
        
#         return results
        
#     except Exception as e:
#         print(f"ERROR: Batch classification parse error: {e}")
#         return default_results

# # ================================================================================================
# # API ENDPOINTS
# # ================================================================================================
# @router.post("/search-image", tags=["Classifyapi"])
# async def search_by_image(
#     file: UploadFile = File(...),
#     session_id: str = Form(default=str(uuid.uuid4()))
# ):
#     """T√¨m ki·∫øm theo ·∫£nh"""
#     file_path = f"./media/temp_{uuid.uuid4()}.jpg"
#     try:
#         # Read file content
#         contents = await file.read()
        
#         # Save to temporary file
#         with open(file_path, "wb") as buffer:
#             buffer.write(contents)
        
#         # Open image using PIL
#         img = Image.open(file_path)
#         model = genai.GenerativeModel("gemini-2.5-flash")
        
#         # prompt = """
#         # ƒê√≥ng vai chuy√™n vi√™n t∆∞ v·∫•n v·∫≠t t∆∞ AA corporation (N·ªôi th·∫•t cao c·∫•p).
#         # Ph√¢n t√≠ch ·∫£nh n·ªôi th·∫•t n√†y ƒë·ªÉ tr√≠ch xu·∫•t th√¥ng tin t√¨m ki·∫øm Database.
        
#         # OUTPUT JSON ONLY (no markdown, no backticks):
#         # {
#         #     "category": "Lo·∫°i SP (B√†n, Gh·∫ø, Sofa, T·ªß, Gi∆∞·ªùng, ƒê√®n, K·ªá...)",
#         #     "visual_description": "M√¥ t·∫£ chi ti·∫øt cho kh√°ch h√†ng hi·ªÉu s·∫£n ph·∫©m",
#         #     "search_keywords": "CH·ªà 1-2 T·ª™ KH√ìA ƒê∆†N GI·∫¢N NH·∫§T (VD: b√†n l√†m vi·ªác, gh·∫ø sofa, t·ªß g·ªó, gi∆∞·ªùng ng·ªß)",
#         #     "material_detected": "V·∫≠t li·ªáu ch√≠nh (G·ªó, Da, V·∫£i, ƒê√°, Kim lo·∫°i...)",
#         #     "color_tone": "M√†u ch·ªß ƒë·∫°o"
#         # }
        
#         # L∆ØU √ù: search_keywords PH·∫¢I C·ª∞C K·ª≤ NG·∫ÆN G·ªåN, CH·ªà T√äN LO·∫†I S·∫¢N PH·∫®M. VD: "b√†n l√†m vi·ªác" KH√îNG PH·∫¢I "b√†n l√†m vi·ªác g·ªó hi·ªán ƒë·∫°i m√†u n√¢u"
#         # """
        
#         prompt = """
#         VAI TR√í (ROLE)
#         B·∫°n l√† Chuy√™n vi√™n Ph√¢n t√≠ch V·∫≠t t∆∞ N·ªôi th·∫•t cao c·∫•p t·∫°i AA Corporation. B·∫°n c√≥ ki·∫øn th·ª©c s√¢u r·ªông v·ªÅ v·∫≠t li·ªáu, k·∫øt c·∫•u v√† phong c√°ch thi·∫øt k·∫ø n·ªôi th·∫•t.

#         NHI·ªÜM V·ª§ (TASK)
#         Ph√¢n t√≠ch h√¨nh ·∫£nh ƒë∆∞·ª£c cung c·∫•p v√† tr√≠ch xu·∫•t th√¥ng tin k·ªπ thu·∫≠t v√†o ƒë·ªãnh d·∫°ng JSON Array (M·∫£ng) chu·∫©n ƒë·ªÉ nh·∫≠p v√†o h·ªá th·ªëng c∆° s·ªü d·ªØ li·ªáu t√¨m ki·∫øm.

#         CHI·∫æN L∆Ø·ª¢C D·ªÆ LI·ªÜU (DATA STRATEGY)
#         Output ph·∫£i l√† m·ªôt m·∫£ng ch·ª©a ch√≠nh x√°c 2 ƒë·ªëi t∆∞·ª£ng (objects) nh·∫±m ph·ª•c v·ª• c∆° ch·∫ø t√¨m ki·∫øm ƒëa t·∫ßng:

#         Object 1 (∆Øu ti√™n): T√¨m ki·∫øm ch√≠nh x√°c (Exact Match). T·ª´ kh√≥a ph·∫£i m√¥ t·∫£ c·ª• th·ªÉ ƒë·∫∑c t√≠nh n·ªïi b·∫≠t nh·∫•t c·ªßa s·∫£n ph·∫©m.

#         Object 2 (D·ª± ph√≤ng): T√¨m ki·∫øm m·ªü r·ªông (Broad Match). T·ª´ kh√≥a l√† danh m·ª•c chung ho·∫∑c t·ª´ ƒë·ªìng nghƒ©a ƒë·ªÉ ƒë·∫£m b·∫£o k·∫øt qu·∫£ t√¨m ki·∫øm kh√¥ng b·ªã r·ªóng n·∫øu t√¨m ch√≠nh x√°c th·∫•t b·∫°i.

#         H∆Ø·ªöNG D·∫™N C√ÅC TR∆Ø·ªúNG (FIELDS)
#         category: Ch·ªâ ch·ªçn 1 danh m·ª•c ch√≠nh x√°c nh·∫•t (VD: Gh·∫ø, B√†n, Sofa, T·ªß, ƒê√®n...).

#         visual_description: Vi·∫øt ƒëo·∫°n vƒÉn m√¥ t·∫£ chuy√™n nghi·ªáp (catalogue). T·∫≠p trung: c·∫•u tr√∫c khung, ch·∫•t li·ªáu b·ªÅ m·∫∑t, t√≠nh nƒÉng v√† c·∫£m gi√°c s·ª≠ d·ª•ng. (N·ªôi dung n√†y gi·ªëng nhau ·ªü c·∫£ 2 object).

#         search_keywords:

#         T·∫°i Object 1: Tr√≠ch xu·∫•t t·ª´ kh√≥a "ng√°ch" c·ª• th·ªÉ (VD: "gh·∫ø xoay l∆∞·ªõi", "sofa da b√≤", "b√†n ƒÉn m·∫∑t ƒë√°").

#         T·∫°i Object 2: Tr√≠ch xu·∫•t t·ª´ kh√≥a "g·ªëc" ph·ªï bi·∫øn (VD: "gh·∫ø vƒÉn ph√≤ng", "sofa ph√≤ng kh√°ch", "b√†n ƒÉn").

#         material_detected: Li·ªát k√™ v·∫≠t li·ªáu nh√¨n th·∫•y, ngƒÉn c√°ch b·∫±ng d·∫•u ph·∫©y. ∆Øu ti√™n t·ª´ chuy√™n ng√†nh (Nh·ª±a PP, Th√©p m·∫° chrome, V·∫£i n·ªâ...).

#         color_tone: M√†u s·∫Øc ch·ªß ƒë·∫°o (T·ªëi ƒëa 2 m√†u).

#         ƒê·ªäNH D·∫†NG OUTPUT (CONSTRAINTS)
#         B·∫Øt bu·ªôc tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng m·∫£ng JSON: [ {...}, {...} ].

#         Kh√¥ng bao b·ªçc b·ªüi markdown (json ... ).

#         Kh√¥ng th√™m l·ªùi d·∫´n hay gi·∫£i th√≠ch.

#         Ng√¥n ng·ªØ: Ti·∫øng Vi·ªát.

#         V√ç D·ª§ M·∫™U (ONE-SHOT EXAMPLE)
#         Input: [H√¨nh ·∫£nh m·ªôt chi·∫øc gh·∫ø vƒÉn ph√≤ng l∆∞·ªõi ƒëen ch√¢n xoay] Output: [ { "category": "Gh·∫ø", "visual_description": "Gh·∫ø xoay vƒÉn ph√≤ng l∆∞ng trung, thi·∫øt k·∫ø khung nh·ª±a ƒë√∫c nguy√™n kh·ªëi k·∫øt h·ª£p l∆∞ng l∆∞·ªõi tho√°ng kh√≠. Tay v·ªãn nh·ª±a c·ªë ƒë·ªãnh d·∫°ng v√≤m. ƒê·ªám ng·ªìi b·ªçc v·∫£i l∆∞·ªõi x·ªëp √™m √°i. Ch√¢n gh·∫ø sao 5 c√°nh b·∫±ng th√©p m·∫° chrome s√°ng b√≥ng, c√≥ b√°nh xe di chuy·ªÉn v√† c·∫ßn g·∫°t ƒëi·ªÅu ch·ªânh ƒë·ªô cao.", "search_keywords": "gh·∫ø xoay l∆∞·ªõi", "material_detected": "L∆∞·ªõi, Nh·ª±a PP, Th√©p m·∫° chrome, V·∫£i, M√∫t", "color_tone": "ƒêen, B·∫°c" }, { "category": "Gh·∫ø", "visual_description": "Gh·∫ø xoay vƒÉn ph√≤ng l∆∞ng trung, thi·∫øt k·∫ø khung nh·ª±a ƒë√∫c nguy√™n kh·ªëi k·∫øt h·ª£p l∆∞ng l∆∞·ªõi tho√°ng kh√≠. Tay v·ªãn nh·ª±a c·ªë ƒë·ªãnh d·∫°ng v√≤m. ƒê·ªám ng·ªìi b·ªçc v·∫£i l∆∞·ªõi x·ªëp √™m √°i. Ch√¢n gh·∫ø sao 5 c√°nh b·∫±ng th√©p m·∫° chrome s√°ng b√≥ng, c√≥ b√°nh xe di chuy·ªÉn v√† c·∫ßn g·∫°t ƒëi·ªÅu ch·ªânh ƒë·ªô cao.", "search_keywords": "gh·∫ø vƒÉn ph√≤ng", "material_detected": "L∆∞·ªõi, Nh·ª±a PP, Th√©p m·∫° chrome, V·∫£i, M√∫t", "color_tone": "ƒêen, B·∫°c" } ]

#         B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH H√åNH ·∫¢NH N√ÄY:
#         [AI s·∫Ω ch·ªù b·∫°n upload ·∫£nh t·∫°i ƒë√¢y]
#         """
        
#         response = model.generate_content([prompt, img])
        
#         # print("response Image analysis response:", response)
        
#         if not response.text:
#             return {
#                 "response": "‚ö†Ô∏è Kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c ·∫£nh. Vui l√≤ng th·ª≠ ·∫£nh kh√°c.",
#                 "products": []
#             }
        
#         clean = response.text.strip()
        
#         if "```json" in clean:
            
#             clean = clean.split("```json")[1].split("```")[0].strip()
#         elif "```" in clean:
#             clean = clean.split("```")[1].split("```")[0].strip()
        
#         try:
#             ai_result = json.loads(clean)
#         except json.JSONDecodeError as e:
#             print(f"JSON Parse Error: {e}")
#             ai_result = {
#                 "visual_description": clean[:200],
#                 "search_keywords": "",
#                 "category": "N·ªôi th·∫•t"
#             }
        
#         # L·∫•y search_keywords v√† r√∫t g·ªçn n·∫øu qu√° d√†i
#         search_keywords = ai_result[0].get("search_keywords", "").strip()
#         category = ai_result[0].get("category", "")
        
#         # N·∫øu search_keywords qu√° d√†i (>50 k√Ω t·ª±) ho·∫∑c r·ªóng, d√πng category
#         if not search_keywords or len(search_keywords) > 50:
#             search_text = category  # Ch·ªâ d√πng category ƒë∆°n gi·∫£n nh·∫•t
#             print(f"INFO: Using category as search term: {search_text}")
#         else:
#             # L·∫•y t·ªëi ƒëa 3 t·ª´ ƒë·∫ßu ti√™n c·ªßa search_keywords
#             words = search_keywords.split()[:3]
#             search_text = " ".join(words)
#             print(f"INFO: Using simplified keywords: {search_text}")
        
#         params = {
#             "category": category,
#             "keywords_vector": search_text,  # T·ª´ kh√≥a C·ª∞C K·ª≤ ƒë∆°n gi·∫£n
#             "material_primary": ai_result[0].get("material_detected")
#         }
        
#         search_result = search_products(params, session_id=session_id)
#         products = search_result.get("products", [])
        
#         # ========== IMAGE MATCHING VALIDATION ==========
#         # Ki·ªÉm tra s·∫£n ph·∫©m c√≥ kh·ªõp v·ªõi ai_interpretation kh√¥ng
#         ai_interpretation = ai_result[0].get("visual_description", "").lower()
        
#         for product in products:
#             product_name = (product.get('product_name') or '').lower()
#             category = (product.get('category') or '').lower()
            
#             # Ki·ªÉm tra t√™n ho·∫∑c danh m·ª•c c√≥ trong ai_interpretation kh√¥ng
#             name_match = any(word in ai_interpretation for word in product_name.split() if len(word) > 2)
#             category_match = category in ai_interpretation
            
#             # N·∫øu kh√¥ng kh·ªõp -> tr·ª´ base_score
#             if not name_match and not category_match:
#                 current_score = product.get('base_score', 0.5)
#                 penalty = 0.25  # Tr·ª´ 0.25 ƒëi·ªÉm
#                 product['base_score'] = max(0, current_score - penalty)
#                 product['image_mismatch'] = True
#                 product['penalty_applied'] = penalty
#                 print(f"  ‚ö†Ô∏è Image mismatch penalty for {product.get('headcode')}: {current_score:.3f} -> {product['base_score']:.3f}")
#             else:
#                 product['image_mismatch'] = False
        
#         # Ph√¢n lo·∫°i s·∫£n ph·∫©m theo base_score
#         products_main = [p for p in products if p.get('base_score', 0) >= 0.7]
#         products_low_confidence = [p for p in products if p.get('base_score', 0) < 0.6]
        
#         print(f"INFO: Image search - Main products: {len(products_main)}, Low confidence: {len(products_low_confidence)}")
        
#         histories.save_chat_to_histories(
#             email="test@gmail.com",
#             session_id=session_id,
#             question="[IMAGE_UPLOAD]",
#             answer=f"Ph√¢n t√≠ch ·∫£nh: {ai_result[0].get('visual_description', 'N/A')[:100]}... | T√¨m th·∫•y {len(products_main)} s·∫£n ph·∫©m (High confidence)"
#         )

#         # N·∫øu kh√¥ng c√≥ s·∫£n ph·∫©m n√†o ƒë·∫°t base_score >= 0.7
#         if not products_main:
#             return {
#                 "response": f"üì∏ **Ph√¢n t√≠ch ·∫£nh:** T√¥i nh·∫≠n th·∫•y ƒë√¢y l√† **{ai_result[0].get('visual_description', 's·∫£n ph·∫©m n·ªôi th·∫•t')}**.\n\n"
#                         f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p v·ªõi y√™u c·∫ßu.\n\n"
#                         f"üí° **G·ª£i √Ω**: B·∫°n c√≥ th·ªÉ m√¥ t·∫£ chi ti·∫øt h∆°n. Ho·∫∑c b·∫°n c√≥ th·ªÉ t√¨m s·∫£n ph·∫©m kh√°c. T√¥i s·∫Ω g·ª£i √Ω cho b·∫°n danh s√°ch s·∫£n ph·∫©m",
#                 "products": None,
#                 "productLowConfidence": products_low_confidence[:5] if products_low_confidence else [],
#                 "ai_interpretation": ai_result[0].get("visual_description", ""),
#                 "search_method": "image_vector"
#             }
        
#         return {
#             "response": f"üì∏ **Ph√¢n t√≠ch ·∫£nh:** T√¥i nh·∫≠n th·∫•y ƒë√¢y l√† **{ai_result[0].get('visual_description', 's·∫£n ph·∫©m')}**.\n\n"
#                        f"‚úÖ ƒê√£ t√¨m th·∫•y **{len(products_main)} s·∫£n ph·∫©m** ph√π h·ª£p:",
#             "products": products_main,
#             "productLowConfidence": products_low_confidence[:5] if products_low_confidence else [],
#             "ai_interpretation": ai_result[0].get("visual_description", ""),
#             "search_method": "image_vector",
#             "confidence_summary": {
#                 "high_confidence": len(products_main),
#                 "low_confidence": len(products_low_confidence)
#             }
#         }
    
#     except Exception as e:
#         print(f"ERROR: Image search error: {e}")
#         import traceback
#         traceback.print_exc()
        
#         return {
#             "response": f"‚ö†Ô∏è L·ªói x·ª≠ l√Ω ·∫£nh: {str(e)}. Vui l√≤ng th·ª≠ l·∫°i.",
#             "products": []
#         }
    
#     finally:
#         if os.path.exists(file_path):
#             try:
#                 os.remove(file_path)
#             except:
#                 pass

# @router.post("/classify-products", tags=["Classifyapi"])
# def classify_pending_products():
#     """
#     ü§ñ Ph√¢n lo·∫°i H√ÄNG LO·∫†T c√°c s·∫£n ph·∫©m ch∆∞a ph√¢n lo·∫°i
#     Batch size: 8 s·∫£n ph·∫©m/l·∫ßn (tr√°nh qu√° d√†i response)
#     """
#     try:
#         conn = get_db()
#         cur = conn.cursor(cursor_factory=RealDictCursor)
        
#         # L·∫•y s·∫£n ph·∫©m ch∆∞a ph√¢n lo·∫°i
#         cur.execute("""
#             SELECT headcode, id_sap, product_name 
#             FROM products_qwen 
#             WHERE category = 'Ch∆∞a ph√¢n lo·∫°i' 
#                 OR sub_category = 'Ch∆∞a ph√¢n lo·∫°i'
#                 OR material_primary = 'Ch∆∞a x√°c ƒë·ªãnh'
#             LIMIT 100
#         """)
        
#         pending_products = cur.fetchall()
        
#         if not pending_products:
#             conn.close()
#             return {
#                 "message": "‚úÖ T·∫•t c·∫£ s·∫£n ph·∫©m ƒë√£ ƒë∆∞·ª£c ph√¢n lo·∫°i!",
#                 "classified": 0,
#                 "total": 0,
#                 "remaining": 0
#             }
        
#         total_pending = len(pending_products)
#         classified = 0
#         errors = []
        
#         BATCH_SIZE = 8  # Gemini x·ª≠ l√Ω t·ªët v·ªõi 5-10 items
        
#         for i in range(0, len(pending_products), BATCH_SIZE):
#             batch = pending_products[i:i+BATCH_SIZE]
            
#             # Chu·∫©n b·ªã input cho batch classification
#             batch_input = [{
#                 'id_sap': p['id_sap'],
#                 'name': p['product_name']
#             } for p in batch]
            
#             print(f"INFO: Classifying batch {i//BATCH_SIZE + 1} ({len(batch)} products)...")
            
#             try:
#                 # G·ªåI BATCH CLASSIFICATION
#                 results = batch_classify_products(batch_input)
                
#                 # C·∫≠p nh·∫≠t v√†o DB
#                 for j, result in enumerate(results):
#                     try:
#                         cur.execute("""
#                             UPDATE products_qwen 
#                             SET category = %s,
#                                 sub_category = %s,
#                                 material_primary = %s,
#                                 updated_at = NOW()
#                             WHERE headcode = %s
#                         """, (
#                             result['category'],
#                             result['sub_category'],
#                             result['material_primary'],
#                             batch[j]['headcode']
#                         ))
#                         classified += 1
#                     except Exception as e:
#                         errors.append(f"{batch[j]['headcode']}: {str(e)[:50]}")
#                 conn.commit()
#                 # Delay gi·ªØa c√°c batch ƒë·ªÉ tr√°nh rate limit
#                 if i + BATCH_SIZE < len(pending_products):
#                     time.sleep(4)
                
#             except Exception as e:
#                 print(f"ERROR: Batch {i//BATCH_SIZE + 1} failed: {e}")
#                 errors.append(f"Batch {i//BATCH_SIZE + 1}: {str(e)[:100]}")
#                 # Ti·∫øp t·ª•c v·ªõi batch ti·∫øp theo
#                 continue
        
#         conn.close()
        
#         # Ki·ªÉm tra c√≤n bao nhi√™u ch∆∞a ph√¢n lo·∫°i
#         conn = get_db()
#         cur = conn.cursor()
#         cur.execute("""
#             SELECT COUNT(*) FROM products_qwen 
#             WHERE category = 'Ch∆∞a ph√¢n lo·∫°i' 
#             OR sub_category = 'Ch∆∞a ph√¢n lo·∫°i'
#             OR material_primary = 'Ch∆∞a x√°c ƒë·ªãnh'
#         """)
#         remaining = cur.fetchone()[0]
#         conn.close()
        
#         return {
#             "message": f"‚úÖ ƒê√£ ph√¢n lo·∫°i {classified}/{total_pending} s·∫£n ph·∫©m",
#             "classified": classified,
#             "total": total_pending,
#             "remaining": remaining,
#             "errors": errors[:10] if errors else []
#         }
        
#     except Exception as e:
#         return {
#             "message": f"‚ùå L·ªói: {str(e)}",
#             "classified": 0,
#             "total": 0,
#             "remaining": 0
#         }

# @router.post("/classify-materials", tags=["Classifyapi"])
# def classify_pending_materials():
#     """
#     ü§ñ Ph√¢n lo·∫°i H√ÄNG LO·∫†T c√°c v·∫≠t li·ªáu ch∆∞a ph√¢n lo·∫°i
#     """
#     try:
#         conn = get_db()
#         cur = conn.cursor(cursor_factory=RealDictCursor)
        
#         cur.execute(f"""
#             SELECT id_sap, material_name, material_group
#             FROM {settings.MATERIALS_TABLE} 
#             WHERE material_subgroup = 'Ch∆∞a ph√¢n lo·∫°i'
#             LIMIT 100
#         """)
        
#         pending_materials = cur.fetchall()
        
#         if not pending_materials:
#             conn.close()
#             return {
#                 "message": "‚úÖ T·∫•t c·∫£ v·∫≠t li·ªáu ƒë√£ ƒë∆∞·ª£c ph√¢n lo·∫°i!",
#                 "classified": 0,
#                 "total": 0,
#                 "remaining": 0
#             }
        
#         total_pending = len(pending_materials)
#         classified = 0
#         errors = []
        
#         BATCH_SIZE = 10
        
#         for i in range(0, len(pending_materials), BATCH_SIZE):
#             batch = pending_materials[i:i+BATCH_SIZE]
            
#             batch_input = [{
#                 'id_sap': m['id_sap'],
#                 'name': m['material_name']
#             } for m in batch]
            
#             print(f"BOT: Classifying materials batch {i//BATCH_SIZE + 1} ({len(batch)} items)...")
            
#             try:
#                 results = batch_classify_materials(batch_input)
                
#                 for j, result in enumerate(results):
#                     try:
#                         cur.execute("""
#                             UPDATE materials 
#                             SET material_subgroup = %s,
#                                 updated_at = NOW()
#                             WHERE id_sap = %s
#                         """, (
#                             result['material_subgroup'],
#                             batch[j]['id_sap']
#                         ))
#                         classified += 1
#                     except Exception as e:
#                         errors.append(f"{batch[j]['id_sap']}: {str(e)[:50]}")
                
#                 conn.commit()
                
#                 if i + BATCH_SIZE < len(pending_materials):
#                     time.sleep(4)
                
#             except Exception as e:
#                 print(f"ERROR: Materials batch {i//BATCH_SIZE + 1} failed: {e}")
#                 errors.append(f"Batch {i//BATCH_SIZE + 1}: {str(e)[:100]}")
#                 continue
        
#         conn.close()
        
#         conn = get_db()
#         cur = conn.cursor()
#         cur.execute(f"""
#             SELECT COUNT(*) FROM {settings.MATERIALS_TABLE} 
#             WHERE material_subgroup = 'Ch∆∞a ph√¢n lo·∫°i'
#         """)
#         remaining = cur.fetchone()[0]
#         conn.close()
        
#         return {
#             "message": f"‚úÖ ƒê√£ ph√¢n lo·∫°i {classified}/{total_pending} v·∫≠t li·ªáu",
#             "classified": classified,
#             "total": total_pending,
#             "remaining": remaining,
#             "errors": errors[:10] if errors else []
#         }
        
#     except Exception as e:
#         return {
#             "message": f"‚ùå L·ªói: {str(e)}",
#             "classified": 0,
#             "total": 0,
#             "remaining": 0
#         }
